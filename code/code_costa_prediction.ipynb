{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costa Rican Household Poverty Level Prediction\n",
    "\n",
    "**This is the course project collaborated by Xian Liu**$^*$**, Ziyue Wu**$^*$**, Xinyuan Xu**$^*$ ***($*$=equal contribution)***\n",
    "\n",
    "**Last updated: March 22nd, 2020**\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?\n",
    "\n",
    "Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It’s especially tricky when a program focuses on the poorest segment of the population. The world’s poorest typically can’t provide the necessary income and expense records to prove that they qualify.\n",
    "\n",
    "In Latin America, one popular method uses an algorithm to verify income qualification. It’s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family’s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.\n",
    "\n",
    "While this is an improvement, accuracy remains a problem as the region’s population grows and poverty declines.\n",
    "\n",
    "To improve on PMT, the IDB (the largest source of development financing for Latin America and the Caribbean) has turned to the Kaggle community. They believe that new methods beyond traditional econometrics, based on a dataset of Costa Rican household characteristics, might help improve PMT’s performance.\n",
    "\n",
    "Beyond Costa Rica, many countries face this same problem of inaccurately assessing social need. If Kagglers can generate an improvement, the new algorithm could be implemented in other countries around the world.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "### Full Description\n",
    "\n",
    "- {train|test}.csv - the training set\n",
    "\n",
    "    - This is the main table, broken into two files for Train (with a Target column) and Test (without the Target column).\n",
    "    - One row represents one person in our data sample.\n",
    "    - Multiple people can be part of a single household. Only predictions for heads of household are scored.\n",
    "\n",
    "\n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "\n",
    "    - This file contains all test IDs and a default value.\n",
    "    - Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored.\n",
    "    \n",
    "### Core Data fields\n",
    "\n",
    "- Id - a unique identifier for each row.\n",
    "- Target - the target is an ordinal variable indicating groups of income levels.\n",
    "    - 1 = extreme poverty\n",
    "    - 2 = moderate poverty\n",
    "    - 3 = vulnerable households\n",
    "    - 4 = non vulnerable households\n",
    "- idhogar - this is a unique identifier for each household. This can be used to create household-wide features, etc. All rows in a given household will have a matching value for this identifier.\n",
    "- parentesco1 - indicates if this person is the head of the household.\n",
    "- This data contains 142 total columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 : Import package and Read data file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Pre-processing\n",
    "\n",
    "### Why should we do pre-processing?\n",
    "\n",
    "- The input data are per-individual, but predictions should be finally made per-household. Technically, the expected output is per-individual as well, but only predictions for head of househould are taken into account. So, it's per-household. Due to this, we will work per-household during predicting.\n",
    "\n",
    "### How to pre-processing?\n",
    "\n",
    "- For the easiest situation, we can simply utilize feature (`mean`, `std`, `max`, `min`, etc.) to represent the features per-household by exerting calculation on each individual of this family. For example, `rooms`, `bedrooms` should be groupping into a certain family.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partly of our code is from https://www.kaggle.com/ashishpatel26/feature-importance-of-lightgbm\n",
    "\n",
    "def preprocess(df):\n",
    "    \n",
    "    def mk_categoricals(df, prefixes=None, subsets=None):\n",
    "\n",
    "        def mk_category(dummies):\n",
    "            assert (dummies.sum(axis=1) <= 1).all()\n",
    "            nans = dummies.sum(axis=1) != 1\n",
    "            if nans.any():\n",
    "                dummies = dummies.assign(_na=nans.astype(int))\n",
    "            return dummies.idxmax(axis=1).astype('category')\n",
    "\n",
    "        categoricals = pd.DataFrame()\n",
    "\n",
    "        if prefixes:\n",
    "            for prefix in prefixes:\n",
    "                columns = df.columns[df.columns.str.startswith(prefix)]\n",
    "                categoricals[prefix] = mk_category(df[columns])\n",
    "        if subsets:\n",
    "            for feature_name, subset in subsets.items():\n",
    "                categoricals[feature_name] = mk_category(df[subset])\n",
    "\n",
    "        return categoricals\n",
    "    \n",
    "    \n",
    "    \n",
    "    # As is mentioned in data description, the field idhogar represent the house id.\n",
    "    # we call the groupby function to make the individuals group by their family id.\n",
    "    family_group = df.groupby('idhogar')\n",
    "    \n",
    "    # the parentesco field indicates who is the head of a family\n",
    "    # category into years of scholar, head of female and head of partner scholar \n",
    "    family_head = (pd.DataFrame(dict(\n",
    "                    head_escolari = df.parentesco1 * df.escolari,\n",
    "                    head_female = df.parentesco1 * df.female,\n",
    "                    head_partner_escolari = df.parentesco2 * df.escolari))\n",
    "                    .groupby(df.idhogar)\n",
    "                    .max())\n",
    "    \n",
    "    # we combine the features by exerting max, mean, std operations\n",
    "    features_combine = (family_group.mean()[['escolari', 'age', 'hogar_nin', \n",
    "                                    'hogar_total', 'epared3', 'epared1','etecho3', 'etecho1', 'eviv3', 'eviv1',\n",
    "                                    'male', 'r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3', 'v2a1', \n",
    "                                    'rooms', 'bedrooms', 'meaneduc', 'SQBdependency', 'rez_esc', 'refrig', \n",
    "                                    'tamviv', 'overcrowding']]\n",
    "                       .join(family_group.std()[['escolari', 'age']], rsuffix='_std')\n",
    "                       .join(family_group[['escolari', 'age']].min(), rsuffix=\"_min\")\n",
    "                       .join(family_group[['escolari', 'age']].max(), rsuffix=\"_max\")\n",
    "                       .join(family_group[['dis']].sum(), rsuffix=\"_sum\")\n",
    "                   \n",
    "                       .assign(child_rate=lambda x: x.hogar_nin / x.hogar_total,\n",
    "                           wrf=lambda x: x.epared3 - x.epared1 +\n",
    "                                         x.etecho3 - x.etecho1 +\n",
    "                                         x.eviv3 - x.eviv1,\n",
    "                           # wrf is an integral feature that measure quality of the house\n",
    "                           escolari_range=lambda x: x.escolari_max - x.escolari_min,\n",
    "                           age_range=lambda x: x.age_max - x.age_min,\n",
    "                           rent_per_individual=lambda x: x.v2a1 / x.r4t3,\n",
    "                           rent_per_child=lambda x: x.v2a1 / x.r4t1,\n",
    "                           rent_per_over65=lambda x: x.v2a1 / x.r4t3,\n",
    "                           rent_per_room=lambda x: x.v2a1 / x.rooms,\n",
    "                           rent_per_bedroom=lambda x: x.v2a1 / x.bedrooms,\n",
    "                           rooms_per_individual=lambda x: x.rooms / x.r4t3,\n",
    "                           rooms_per_child=lambda x: x.rooms / x.r4t1,\n",
    "                           bedrooms_per_individual=lambda x: x.bedrooms / x.r4t3,\n",
    "                           bedrooms_per_child=lambda x: x.bedrooms / x.r4t1,\n",
    "                           years_schooling_per_individual=lambda x: x.escolari / x.r4t3,\n",
    "                           years_schooling_per_adult=lambda x: x.escolari / (x.r4t3 - x.r4t1),\n",
    "                           years_schooling_per_child=lambda x: x.escolari / x.r4t3\n",
    "                          )\n",
    "                       .drop(['hogar_nin', 'hogar_total', 'epared3', 'epared1',\n",
    "                                   'etecho3', 'etecho1', 'eviv3', 'eviv1'], \n",
    "                             axis=1)\n",
    "                       .join(family_head)\n",
    "                       .join(family_group[['computer', 'television', \n",
    "                                   'qmobilephone', 'v18q1']]\n",
    "                        .mean().sum(axis=1).rename('technics'))\n",
    "                   # we provide integral technical level as a new feature \n",
    "                       .assign(technics_per_individual=lambda x: x.technics / x.r4t3,\n",
    "                               technics_per_child=lambda x: x.technics / x.r4t1)\n",
    "                       .join(mk_categoricals(family_group.mean(), \n",
    "                                prefixes=['lugar', 'area', 'tipovivi', \n",
    "                                          'energcocinar', \n",
    "                                          'sanitario', 'pared', 'piso',\n",
    "                                          'abastagua'],\n",
    "                                subsets={'electricity': ['public', \n",
    "                                                         'planpri', \n",
    "                                                         'noelec', \n",
    "                                                         'coopele']}))\n",
    "                  )\n",
    "    return features_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d3c7b697fabc249fb1b80db62758dcaad3b09615",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = preprocess(df)\n",
    "y = df.groupby('idhogar').Target.mean().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Utilize model to solve the problem\n",
    "\n",
    "### Which model?\n",
    "\n",
    "- We utilize the LightGMB classifier to solve this problem.\n",
    "\n",
    "### Training strategy?\n",
    "\n",
    "- Some strategies mentioned in our class: dropout, mini_batch gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "abdb900b639f0bf5350cfd1bbd220b8448241a6c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(class_weight='balanced',drop_rate=0.9, min_data_in_leaf=100, max_bin=255,\n",
    "                                 n_estimators=500,min_sum_hessian_in_leaf=1,importance_type='gain',learning_rate=0.1,bagging_fraction = 0.85,\n",
    "                                 colsample_bytree = 1.0,feature_fraction = 0.1,lambda_l1 = 5.0,lambda_l2 = 3.0,max_depth =  9,\n",
    "                                 min_child_samples = 55,min_child_weight = 5.0,min_split_gain = 0.1,num_leaves = 45,subsample = 0.75)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "268e840f90ff2eae91262abffc69207066bcb01a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../input/test.csv\").set_index('Id')\n",
    "X_test = preprocess(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Training Strategy\n",
    "\n",
    "### How to divide the data set?\n",
    "\n",
    "- We utilize the method of cross-validation, which divided the data set into 5 folds, with each time 4 folds being training set and 1 fold being validation set.\n",
    "\n",
    "### Training strategy?\n",
    "\n",
    "- Early stopping, which has been mentioned in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c013c1840d2126c11411a01394ab92b68e69765",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# partially based on https://www.kaggle.com/c0conuts/xgb-k-folds-fastai-pca\n",
    "predicts = []\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print(\"###\")\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "            early_stopping_rounds=20)\n",
    "    predicts.append(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce14b2913f725f6cd6e1a33fe3d3064d6cfecdc9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_by_hh = pd.DataFrame(np.array(predicts).mean(axis=0).round().astype(int),\n",
    "                             columns=['Target'],\n",
    "                             index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ace74699d67c4ddb2a369436d55edff82578eb81",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = df_test.join(predict_by_hh, on='idhogar')[['Target']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "998e033149ff68cae360d05ed3838cc745d954e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Visualize the feature importance\n",
    "\n",
    "- We utilize the matplotlib package to visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fdac22f68e9c37ee48fda8546c1c312e80f8e78c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\n",
    "feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,X.columns)), columns=['Value','Feature'])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('lgbm_importances-01.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
